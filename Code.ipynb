{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Fashion_MNIST_Classification_PyTorch.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/moustafa-7/Fashion_MNIST_Classification_PyTorch/blob/master/Code.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gyF86kDh_rp0",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 171
        },
        "outputId": "e21fecc0-3753-4295-90cb-ccb21c0bfc2b"
      },
      "source": [
        "!pip install GPUtil "
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting GPUtil\n",
            "  Downloading https://files.pythonhosted.org/packages/ed/0e/5c61eedde9f6c87713e89d794f01e378cfd9565847d4576fa627d758c554/GPUtil-1.4.0.tar.gz\n",
            "Building wheels for collected packages: GPUtil\n",
            "  Building wheel for GPUtil (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Stored in directory: /root/.cache/pip/wheels/3d/77/07/80562de4bb0786e5ea186911a2c831fdd0018bda69beab71fd\n",
            "Successfully built GPUtil\n",
            "Installing collected packages: GPUtil\n",
            "Successfully installed GPUtil-1.4.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cJcCbOeKvww5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "import torch \n",
        "import matplotlib.pyplot as plt\n",
        "import time"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9QiFVmvGwNu7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from torchvision import datasets, transforms\n",
        "\n",
        "# Defining Transforms for normalization\n",
        "\n",
        "transforms = transforms.Compose([transforms.ToTensor(),transforms.Normalize((0.5,),(0.5,))])\n",
        "\n",
        "trainset = datasets.FashionMNIST('MNIST_data/',download = True, train = True, transform = transforms)\n",
        "testset = datasets.FashionMNIST('MNIST_data/',download = True, train = False, transform = transforms)\n",
        "\n",
        "trainloader = torch.utils.data.DataLoader(trainset, batch_size = 64, shuffle = True)\n",
        "testloader = torch.utils.data.DataLoader(testset, batch_size = 64, shuffle = True)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "S4Y_Bq-4zkL_",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 67
        },
        "outputId": "1290c1ff-687c-47f2-ff64-ad04c346067c"
      },
      "source": [
        "iter = iter(trainloader)\n",
        "images, labels = iter.next()\n",
        "print(type(images))\n",
        "print(images.shape)\n",
        "print(labels.shape)"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "<class 'torch.Tensor'>\n",
            "torch.Size([64, 1, 28, 28])\n",
            "torch.Size([64])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UxFvSFoC0vPa",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 286
        },
        "outputId": "ce928537-84a8-4962-d2d4-965577b357e0"
      },
      "source": [
        "plt.imshow(images[1].numpy().squeeze(), cmap = 'Greys_r')"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.image.AxesImage at 0x7ff06d6daf98>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 10
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAP8AAAD8CAYAAAC4nHJkAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nTxTU/wPefivSHtXDfAAAAABJRU5ErkJggg==\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ti7Jv0b-_lUb",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 50
        },
        "outputId": "4cd0a132-0585-420e-9734-206bb567265c"
      },
      "source": [
        "\n",
        "# GPU check\n",
        "import psutil\n",
        "import humanize\n",
        "import os\n",
        "import GPUtil as GPU\n",
        "GPUs = GPU.getGPUs()\n",
        "# XXX: only one GPU on Colab and isnâ€™t guaranteed\n",
        "gpu = GPUs[0]\n",
        "def printm():\n",
        " process = psutil.Process(os.getpid())\n",
        " print(\"Gen RAM Free: \" + humanize.naturalsize( psutil.virtual_memory().available ), \" | Proc size: \" + humanize.naturalsize( process.memory_info().rss))\n",
        " print(\"GPU RAM Free: {0:.0f}MB | Used: {1:.0f}MB | Util {2:3.0f}% | Total {3:.0f}MB\".format(gpu.memoryFree, gpu.memoryUsed, gpu.memoryUtil*100, gpu.memoryTotal))\n",
        "printm()"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Gen RAM Free: 12.8 GB  | Proc size: 412.3 MB\n",
            "GPU RAM Free: 11430MB | Used: 11MB | Util   0% | Total 11441MB\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Bl_MQVNaVx-I",
        "colab_type": "text"
      },
      "source": [
        "**Normal Model**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sshX4wVAFoNI",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 538
        },
        "outputId": "0c9d80fc-8dd1-4e1f-cc87-685099f5ddf5"
      },
      "source": [
        "# Define the network architecture\n",
        "from torch import nn, optim\n",
        "import torch.nn.functional as F\n",
        "trainloader = torch.utils.data.DataLoader(trainset, batch_size = 64, shuffle = True)\n",
        "testloader = torch.utils.data.DataLoader(testset, batch_size = 64, shuffle = True)\n",
        "\n",
        "start_time = time.time()\n",
        "model = nn.Sequential(nn.Linear(784, 256),\n",
        "                      nn.ReLU(),\n",
        "                      nn.Linear(256, 128),\n",
        "                      nn.ReLU(),\n",
        "                      nn.Linear(128, 64),\n",
        "                      nn.ReLU(),\n",
        "                      nn.Linear(64, 10),\n",
        "                      nn.LogSoftmax(dim = 1)\n",
        "                     )\n",
        "\n",
        "# Define the loss\n",
        "criterion = nn.NLLLoss()\n",
        "\n",
        "# Define the optimizer\n",
        "optimizer = optim.Adam(model.parameters(), lr = 0.002)\n",
        "\n",
        "# Define the epochs\n",
        "epochs = 30\n",
        "\n",
        "train_losses, test_losses = [], []\n",
        "\n",
        "for e in range(epochs):\n",
        "  running_loss = 0\n",
        "  for images, labels in trainloader:\n",
        "    # Flatten Fashion-MNIST images into a 784 (28 * 28) long vector\n",
        "    images = images.view(images.shape[0], -1)\n",
        "    \n",
        "    # Training pass\n",
        "    optimizer.zero_grad()\n",
        "    \n",
        "    output = model.forward(images)\n",
        "    loss = criterion(output, labels)\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "    \n",
        "    running_loss += loss.item()\n",
        "  else:\n",
        "    test_loss = 0\n",
        "    accuracy = 0\n",
        "    \n",
        "    # Turn off gradients for validation, saves memory and computation\n",
        "    with torch.no_grad():\n",
        "      # Set the model to evaluation mode\n",
        "      model.eval()\n",
        "      \n",
        "      # Validation pass\n",
        "      for images, labels in testloader:\n",
        "        images = images.view(images.shape[0], -1)\n",
        "        log_ps = model(images)\n",
        "        test_loss += criterion(log_ps, labels)\n",
        "        \n",
        "        ps = torch.exp(log_ps)\n",
        "        top_p, top_class = ps.topk(1, dim = 1)\n",
        "        equals = top_class == labels.view(*top_class.shape)\n",
        "        accuracy += torch.mean(equals.type(torch.FloatTensor))\n",
        "    \n",
        "    model.train()\n",
        "    train_losses.append(running_loss/len(trainloader))\n",
        "    test_losses.append(test_loss/len(testloader))\n",
        "    \n",
        "    print(\"Epoch: {}/{}..\".format(e+1, epochs),\n",
        "          \"Training loss: {:.3f}..\".format(running_loss/len(trainloader)),\n",
        "          \"Test loss: {:.3f}..\".format(test_loss/len(testloader)),\n",
        "          \"Test Accuracy: {:.3f}\".format(accuracy/len(testloader)))\n",
        "    \n",
        "    \n",
        "    \n",
        "print(\"--- %s seconds ---\" % (time.time() - start_time))"
      ],
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch: 1/30.. Training loss: 0.511.. Test loss: 0.436.. Test Accuracy: 0.840\n",
            "Epoch: 2/30.. Training loss: 0.380.. Test loss: 0.391.. Test Accuracy: 0.861\n",
            "Epoch: 3/30.. Training loss: 0.342.. Test loss: 0.385.. Test Accuracy: 0.856\n",
            "Epoch: 4/30.. Training loss: 0.321.. Test loss: 0.370.. Test Accuracy: 0.867\n",
            "Epoch: 5/30.. Training loss: 0.304.. Test loss: 0.366.. Test Accuracy: 0.871\n",
            "Epoch: 6/30.. Training loss: 0.289.. Test loss: 0.363.. Test Accuracy: 0.871\n",
            "Epoch: 7/30.. Training loss: 0.274.. Test loss: 0.346.. Test Accuracy: 0.872\n",
            "Epoch: 8/30.. Training loss: 0.267.. Test loss: 0.350.. Test Accuracy: 0.879\n",
            "Epoch: 9/30.. Training loss: 0.254.. Test loss: 0.341.. Test Accuracy: 0.880\n",
            "Epoch: 10/30.. Training loss: 0.245.. Test loss: 0.335.. Test Accuracy: 0.880\n",
            "Epoch: 11/30.. Training loss: 0.235.. Test loss: 0.354.. Test Accuracy: 0.884\n",
            "Epoch: 12/30.. Training loss: 0.228.. Test loss: 0.368.. Test Accuracy: 0.883\n",
            "Epoch: 13/30.. Training loss: 0.222.. Test loss: 0.363.. Test Accuracy: 0.884\n",
            "Epoch: 14/30.. Training loss: 0.216.. Test loss: 0.361.. Test Accuracy: 0.884\n",
            "Epoch: 15/30.. Training loss: 0.209.. Test loss: 0.366.. Test Accuracy: 0.883\n",
            "Epoch: 16/30.. Training loss: 0.206.. Test loss: 0.361.. Test Accuracy: 0.887\n",
            "Epoch: 17/30.. Training loss: 0.199.. Test loss: 0.373.. Test Accuracy: 0.884\n",
            "Epoch: 18/30.. Training loss: 0.196.. Test loss: 0.365.. Test Accuracy: 0.885\n",
            "Epoch: 19/30.. Training loss: 0.192.. Test loss: 0.413.. Test Accuracy: 0.883\n",
            "Epoch: 20/30.. Training loss: 0.183.. Test loss: 0.383.. Test Accuracy: 0.885\n",
            "Epoch: 21/30.. Training loss: 0.176.. Test loss: 0.418.. Test Accuracy: 0.888\n",
            "Epoch: 22/30.. Training loss: 0.172.. Test loss: 0.384.. Test Accuracy: 0.886\n",
            "Epoch: 23/30.. Training loss: 0.172.. Test loss: 0.412.. Test Accuracy: 0.884\n",
            "Epoch: 24/30.. Training loss: 0.166.. Test loss: 0.399.. Test Accuracy: 0.886\n",
            "Epoch: 25/30.. Training loss: 0.168.. Test loss: 0.396.. Test Accuracy: 0.887\n",
            "Epoch: 26/30.. Training loss: 0.155.. Test loss: 0.412.. Test Accuracy: 0.889\n",
            "Epoch: 27/30.. Training loss: 0.156.. Test loss: 0.417.. Test Accuracy: 0.889\n",
            "Epoch: 28/30.. Training loss: 0.159.. Test loss: 0.438.. Test Accuracy: 0.890\n",
            "Epoch: 29/30.. Training loss: 0.150.. Test loss: 0.435.. Test Accuracy: 0.886\n",
            "Epoch: 30/30.. Training loss: 0.142.. Test loss: 0.418.. Test Accuracy: 0.889\n",
            "--- 567.227855682373 seconds ---\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Hsq908lVV6k0",
        "colab_type": "text"
      },
      "source": [
        "**Adding Batch Normalization**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iyvk_GSQ9rTC",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 538
        },
        "outputId": "b7231abd-ab00-47cb-923f-e1229e8d1b30"
      },
      "source": [
        "# Define the network architecture\n",
        "from torch import nn, optim\n",
        "import torch.nn.functional as F\n",
        "\n",
        "trainloader = torch.utils.data.DataLoader(trainset, batch_size = 64, shuffle = True)\n",
        "testloader = torch.utils.data.DataLoader(testset, batch_size = 64, shuffle = True)\n",
        "\n",
        "start_time = time.time()\n",
        "\n",
        "model = nn.Sequential(nn.Linear(784, 256),\n",
        "                      nn.BatchNorm1d(256),\n",
        "                      nn.ReLU(),\n",
        "                      nn.Dropout(0.2),\n",
        "                      nn.Linear(256, 128),\n",
        "                      nn.BatchNorm1d(128),\n",
        "                      nn.ReLU(),\n",
        "                      nn.Dropout(0.2),\n",
        "                      nn.Linear(128, 64),\n",
        "                      nn.BatchNorm1d(64),\n",
        "                      nn.ReLU(),\n",
        "                      nn.Dropout(0.2),\n",
        "                      nn.Linear(64, 10),\n",
        "                      nn.LogSoftmax(dim = 1)\n",
        "                     )\n",
        "\n",
        "# Define the loss\n",
        "criterion = nn.NLLLoss()\n",
        "#criterion = nn.CrossEntropyLoss() we can use this one\n",
        "\n",
        "# Define the optimizer\n",
        "optimizer = optim.Adam(model.parameters(), lr = 0.002)\n",
        "\n",
        "# Define the epochs\n",
        "epochs = 30\n",
        "\n",
        "train_losses, test_losses = [], []\n",
        "\n",
        "for e in range(epochs):\n",
        "  running_loss = 0\n",
        "  for images, labels in trainloader:\n",
        "    # Flatten Fashion-MNIST images into a 784 long vector\n",
        "    images = images.view(images.shape[0], -1)\n",
        "    \n",
        "    # Training pass\n",
        "    optimizer.zero_grad()\n",
        "    \n",
        "    output = model.forward(images)\n",
        "    loss = criterion(output, labels)\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "    \n",
        "    running_loss += loss.item()\n",
        "  else:\n",
        "    test_loss = 0\n",
        "    accuracy = 0\n",
        "    \n",
        "    # Turn off gradients for validation, saves memory and computation\n",
        "    with torch.no_grad():\n",
        "      # Set the model to evaluation mode\n",
        "      model.eval()\n",
        "      \n",
        "      # Validation pass\n",
        "      for images, labels in testloader:\n",
        "        images = images.view(images.shape[0], -1)\n",
        "        log_ps = model(images)\n",
        "        test_loss += criterion(log_ps, labels)\n",
        "        \n",
        "        ps = torch.exp(log_ps)\n",
        "        top_p, top_class = ps.topk(1, dim = 1)\n",
        "        equals = top_class == labels.view(*top_class.shape)\n",
        "        accuracy += torch.mean(equals.type(torch.FloatTensor))\n",
        "    \n",
        "    model.train()\n",
        "    train_losses.append(running_loss/len(trainloader))\n",
        "    test_losses.append(test_loss/len(testloader))\n",
        "    \n",
        "    print(\"Epoch: {}/{}..\".format(e+1, epochs),\n",
        "          \"Training loss: {:.3f}..\".format(running_loss/len(trainloader)),\n",
        "          \"Test loss: {:.3f}..\".format(test_loss/len(testloader)),\n",
        "          \"Test Accuracy: {:.3f}\".format(accuracy/len(testloader)))\n",
        "    \n",
        "    \n",
        "    \n",
        "print(\"--- %s seconds with batchnormalization added ---\" % (time.time() - start_time))"
      ],
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch: 1/30.. Training loss: 0.571.. Test loss: 0.422.. Test Accuracy: 0.845\n",
            "Epoch: 2/30.. Training loss: 0.409.. Test loss: 0.365.. Test Accuracy: 0.865\n",
            "Epoch: 3/30.. Training loss: 0.371.. Test loss: 0.352.. Test Accuracy: 0.872\n",
            "Epoch: 4/30.. Training loss: 0.348.. Test loss: 0.334.. Test Accuracy: 0.879\n",
            "Epoch: 5/30.. Training loss: 0.333.. Test loss: 0.342.. Test Accuracy: 0.877\n",
            "Epoch: 6/30.. Training loss: 0.314.. Test loss: 0.330.. Test Accuracy: 0.882\n",
            "Epoch: 7/30.. Training loss: 0.304.. Test loss: 0.318.. Test Accuracy: 0.885\n",
            "Epoch: 8/30.. Training loss: 0.293.. Test loss: 0.317.. Test Accuracy: 0.887\n",
            "Epoch: 9/30.. Training loss: 0.282.. Test loss: 0.307.. Test Accuracy: 0.889\n",
            "Epoch: 10/30.. Training loss: 0.273.. Test loss: 0.307.. Test Accuracy: 0.892\n",
            "Epoch: 11/30.. Training loss: 0.269.. Test loss: 0.317.. Test Accuracy: 0.887\n",
            "Epoch: 12/30.. Training loss: 0.259.. Test loss: 0.301.. Test Accuracy: 0.894\n",
            "Epoch: 13/30.. Training loss: 0.255.. Test loss: 0.311.. Test Accuracy: 0.889\n",
            "Epoch: 14/30.. Training loss: 0.248.. Test loss: 0.301.. Test Accuracy: 0.893\n",
            "Epoch: 15/30.. Training loss: 0.243.. Test loss: 0.311.. Test Accuracy: 0.891\n",
            "Epoch: 16/30.. Training loss: 0.235.. Test loss: 0.305.. Test Accuracy: 0.896\n",
            "Epoch: 17/30.. Training loss: 0.231.. Test loss: 0.298.. Test Accuracy: 0.895\n",
            "Epoch: 18/30.. Training loss: 0.224.. Test loss: 0.302.. Test Accuracy: 0.893\n",
            "Epoch: 19/30.. Training loss: 0.223.. Test loss: 0.293.. Test Accuracy: 0.899\n",
            "Epoch: 20/30.. Training loss: 0.218.. Test loss: 0.308.. Test Accuracy: 0.896\n",
            "Epoch: 21/30.. Training loss: 0.212.. Test loss: 0.297.. Test Accuracy: 0.896\n",
            "Epoch: 22/30.. Training loss: 0.210.. Test loss: 0.303.. Test Accuracy: 0.897\n",
            "Epoch: 23/30.. Training loss: 0.205.. Test loss: 0.302.. Test Accuracy: 0.893\n",
            "Epoch: 24/30.. Training loss: 0.204.. Test loss: 0.297.. Test Accuracy: 0.894\n",
            "Epoch: 25/30.. Training loss: 0.198.. Test loss: 0.297.. Test Accuracy: 0.897\n",
            "Epoch: 26/30.. Training loss: 0.195.. Test loss: 0.300.. Test Accuracy: 0.894\n",
            "Epoch: 27/30.. Training loss: 0.193.. Test loss: 0.294.. Test Accuracy: 0.899\n",
            "Epoch: 28/30.. Training loss: 0.188.. Test loss: 0.306.. Test Accuracy: 0.898\n",
            "Epoch: 29/30.. Training loss: 0.189.. Test loss: 0.300.. Test Accuracy: 0.900\n",
            "Epoch: 30/30.. Training loss: 0.182.. Test loss: 0.302.. Test Accuracy: 0.898\n",
            "--- 636.7784621715546 seconds with batchnormalization added ---\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_7WMJuyZWCKQ",
        "colab_type": "text"
      },
      "source": [
        "**Adding Dropout**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PW0Lccql9g34",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 538
        },
        "outputId": "7135cc6e-aa6e-47bf-e8fb-44225acb04e2"
      },
      "source": [
        "# Define the network architecture\n",
        "from torch import nn, optim\n",
        "import torch.nn.functional as F\n",
        "\n",
        "trainloader = torch.utils.data.DataLoader(trainset, batch_size = 64, shuffle = True)\n",
        "testloader = torch.utils.data.DataLoader(testset, batch_size = 64, shuffle = True)\n",
        "\n",
        "start_time = time.time()\n",
        "model = nn.Sequential(nn.Linear(784, 256),\n",
        "                      nn.ReLU(),\n",
        "                      nn.Dropout(0.2),\n",
        "                      nn.Linear(256, 128),\n",
        "                      nn.ReLU(),\n",
        "                      nn.Dropout(0.2),\n",
        "                      nn.Linear(128, 64),\n",
        "                      nn.ReLU(),\n",
        "                      nn.Dropout(0.2),\n",
        "                      nn.Linear(64, 10),\n",
        "                      nn.LogSoftmax(dim = 1)\n",
        "                     )\n",
        "\n",
        "# Define the loss\n",
        "criterion = nn.NLLLoss()\n",
        "\n",
        "# Define the optimizer\n",
        "optimizer = optim.Adam(model.parameters(), lr = 0.002)\n",
        "\n",
        "# Define the epochs\n",
        "epochs = 30\n",
        "\n",
        "train_losses, test_losses = [], []\n",
        "\n",
        "for e in range(epochs):\n",
        "  running_loss = 0\n",
        "  for images, labels in trainloader:\n",
        "    # Flatten Fashion-MNIST images into a 784 long vector\n",
        "    images = images.view(images.shape[0], -1)\n",
        "    \n",
        "    # Training pass\n",
        "    optimizer.zero_grad()\n",
        "    \n",
        "    output = model.forward(images)\n",
        "    loss = criterion(output, labels)\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "    \n",
        "    running_loss += loss.item()\n",
        "  else:\n",
        "    test_loss = 0\n",
        "    accuracy = 0\n",
        "    \n",
        "    # Turn off gradients for validation, saves memory and computation\n",
        "    with torch.no_grad():\n",
        "      # Set the model to evaluation mode\n",
        "      model.eval()\n",
        "      \n",
        "      # Validation pass\n",
        "      for images, labels in testloader:\n",
        "        images = images.view(images.shape[0], -1)\n",
        "        log_ps = model(images)\n",
        "        test_loss += criterion(log_ps, labels)\n",
        "        \n",
        "        ps = torch.exp(log_ps)\n",
        "        top_p, top_class = ps.topk(1, dim = 1)\n",
        "        equals = top_class == labels.view(*top_class.shape)\n",
        "        accuracy += torch.mean(equals.type(torch.FloatTensor))\n",
        "    \n",
        "    model.train()\n",
        "    train_losses.append(running_loss/len(trainloader))\n",
        "    test_losses.append(test_loss/len(testloader))\n",
        "    \n",
        "    print(\"Epoch: {}/{}..\".format(e+1, epochs),\n",
        "          \"Training loss: {:.3f}..\".format(running_loss/len(trainloader)),\n",
        "          \"Test loss: {:.3f}..\".format(test_loss/len(testloader)),\n",
        "          \"Test Accuracy: {:.3f}\".format(accuracy/len(testloader)))\n",
        "    \n",
        "    \n",
        "    \n",
        "print(\"--- %s seconds with dropout added ---\" % (time.time() - start_time))"
      ],
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch: 1/30.. Training loss: 0.592.. Test loss: 0.479.. Test Accuracy: 0.825\n",
            "Epoch: 2/30.. Training loss: 0.454.. Test loss: 0.409.. Test Accuracy: 0.849\n",
            "Epoch: 3/30.. Training loss: 0.422.. Test loss: 0.428.. Test Accuracy: 0.849\n",
            "Epoch: 4/30.. Training loss: 0.397.. Test loss: 0.402.. Test Accuracy: 0.859\n",
            "Epoch: 5/30.. Training loss: 0.385.. Test loss: 0.382.. Test Accuracy: 0.860\n",
            "Epoch: 6/30.. Training loss: 0.371.. Test loss: 0.376.. Test Accuracy: 0.867\n",
            "Epoch: 7/30.. Training loss: 0.363.. Test loss: 0.377.. Test Accuracy: 0.863\n",
            "Epoch: 8/30.. Training loss: 0.357.. Test loss: 0.371.. Test Accuracy: 0.870\n",
            "Epoch: 9/30.. Training loss: 0.349.. Test loss: 0.381.. Test Accuracy: 0.865\n",
            "Epoch: 10/30.. Training loss: 0.345.. Test loss: 0.366.. Test Accuracy: 0.867\n",
            "Epoch: 11/30.. Training loss: 0.338.. Test loss: 0.358.. Test Accuracy: 0.874\n",
            "Epoch: 12/30.. Training loss: 0.333.. Test loss: 0.360.. Test Accuracy: 0.876\n",
            "Epoch: 13/30.. Training loss: 0.327.. Test loss: 0.364.. Test Accuracy: 0.871\n",
            "Epoch: 14/30.. Training loss: 0.325.. Test loss: 0.374.. Test Accuracy: 0.868\n",
            "Epoch: 15/30.. Training loss: 0.317.. Test loss: 0.348.. Test Accuracy: 0.879\n",
            "Epoch: 16/30.. Training loss: 0.316.. Test loss: 0.343.. Test Accuracy: 0.877\n",
            "Epoch: 17/30.. Training loss: 0.313.. Test loss: 0.353.. Test Accuracy: 0.879\n",
            "Epoch: 18/30.. Training loss: 0.311.. Test loss: 0.361.. Test Accuracy: 0.875\n",
            "Epoch: 19/30.. Training loss: 0.307.. Test loss: 0.353.. Test Accuracy: 0.878\n",
            "Epoch: 20/30.. Training loss: 0.309.. Test loss: 0.359.. Test Accuracy: 0.872\n",
            "Epoch: 21/30.. Training loss: 0.303.. Test loss: 0.355.. Test Accuracy: 0.875\n",
            "Epoch: 22/30.. Training loss: 0.296.. Test loss: 0.351.. Test Accuracy: 0.879\n",
            "Epoch: 23/30.. Training loss: 0.303.. Test loss: 0.367.. Test Accuracy: 0.879\n",
            "Epoch: 24/30.. Training loss: 0.295.. Test loss: 0.367.. Test Accuracy: 0.874\n",
            "Epoch: 25/30.. Training loss: 0.298.. Test loss: 0.346.. Test Accuracy: 0.876\n",
            "Epoch: 26/30.. Training loss: 0.291.. Test loss: 0.362.. Test Accuracy: 0.872\n",
            "Epoch: 27/30.. Training loss: 0.288.. Test loss: 0.351.. Test Accuracy: 0.881\n",
            "Epoch: 28/30.. Training loss: 0.289.. Test loss: 0.350.. Test Accuracy: 0.877\n",
            "Epoch: 29/30.. Training loss: 0.285.. Test loss: 0.349.. Test Accuracy: 0.880\n",
            "Epoch: 30/30.. Training loss: 0.288.. Test loss: 0.350.. Test Accuracy: 0.883\n",
            "--- 594.2385289669037 seconds with dropout added ---\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Fvzk3dmTCYm_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dHJuMij2Tzj4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Adjusting the batch size using 32 and 128 this time"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FsjbgdoxU0vV",
        "colab_type": "text"
      },
      "source": [
        " **The 32 batch size**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1kkqYkvtUDBM",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 151
        },
        "outputId": "c59274ce-a104-4333-ed9c-aef63d056188"
      },
      "source": [
        "# Define the network architecture\n",
        "from torch import nn, optim\n",
        "import torch.nn.functional as F\n",
        "\n",
        "trainloader = torch.utils.data.DataLoader(trainset, batch_size = 32, shuffle = True)\n",
        "testloader = torch.utils.data.DataLoader(testset, batch_size = 32, shuffle = True)\n",
        "\n",
        "start_time = time.time()\n",
        "\n",
        "model = nn.Sequential(nn.Linear(784, 256),\n",
        "                      nn.BatchNorm1d(256),\n",
        "                      nn.ReLU(),\n",
        "                      nn.Dropout(0.2),\n",
        "                      nn.Linear(256, 128),\n",
        "                      nn.BatchNorm1d(128),\n",
        "                      nn.ReLU(),\n",
        "                      nn.Dropout(0.2),\n",
        "                      nn.Linear(128, 64),\n",
        "                      nn.BatchNorm1d(64),\n",
        "                      nn.ReLU(),\n",
        "                      nn.Dropout(0.2),\n",
        "                      nn.Linear(64, 10),\n",
        "                      nn.LogSoftmax(dim = 1)\n",
        "                     )\n",
        "\n",
        "# Define the loss\n",
        "criterion = nn.NLLLoss()\n",
        "#criterion = nn.CrossEntropyLoss() we can use this one\n",
        "\n",
        "# Define the optimizer\n",
        "optimizer = optim.Adam(model.parameters(), lr = 0.002)\n",
        "\n",
        "# Define the epochs\n",
        "epochs = 30\n",
        "\n",
        "train_losses, test_losses = [], []\n",
        "\n",
        "for e in range(epochs):\n",
        "  running_loss = 0\n",
        "  for images, labels in trainloader:\n",
        "    # Flatten Fashion-MNIST images into a 784 long vector\n",
        "    images = images.view(images.shape[0], -1)\n",
        "    \n",
        "    # Training pass\n",
        "    optimizer.zero_grad()\n",
        "    \n",
        "    output = model.forward(images)\n",
        "    loss = criterion(output, labels)\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "    \n",
        "    running_loss += loss.item()\n",
        "  else:\n",
        "    test_loss = 0\n",
        "    accuracy = 0\n",
        "    \n",
        "    # Turn off gradients for validation, saves memory and computation\n",
        "    with torch.no_grad():\n",
        "      # Set the model to evaluation mode\n",
        "      model.eval()\n",
        "      \n",
        "      # Validation pass\n",
        "      for images, labels in testloader:\n",
        "        images = images.view(images.shape[0], -1)\n",
        "        log_ps = model(images)\n",
        "        test_loss += criterion(log_ps, labels)\n",
        "        \n",
        "        ps = torch.exp(log_ps)\n",
        "        top_p, top_class = ps.topk(1, dim = 1)\n",
        "        equals = top_class == labels.view(*top_class.shape)\n",
        "        accuracy += torch.mean(equals.type(torch.FloatTensor))\n",
        "    \n",
        "    model.train()\n",
        "    train_losses.append(running_loss/len(trainloader))\n",
        "    test_losses.append(test_loss/len(testloader))\n",
        "    \n",
        "    print(\"Epoch: {}/{}..\".format(e+1, epochs),\n",
        "          \"Training loss: {:.3f}..\".format(running_loss/len(trainloader)),\n",
        "          \"Test loss: {:.3f}..\".format(test_loss/len(testloader)),\n",
        "          \"Test Accuracy: {:.3f}\".format(accuracy/len(testloader)))\n",
        "    \n",
        "    \n",
        "    \n",
        "print(\"--- %s seconds with batch size = 32  ---\" % (time.time() - start_time))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch: 1/30.. Training loss: 0.575.. Test loss: 0.416.. Test Accuracy: 0.847\n",
            "Epoch: 2/30.. Training loss: 0.435.. Test loss: 0.368.. Test Accuracy: 0.866\n",
            "Epoch: 3/30.. Training loss: 0.396.. Test loss: 0.361.. Test Accuracy: 0.872\n",
            "Epoch: 4/30.. Training loss: 0.373.. Test loss: 0.342.. Test Accuracy: 0.875\n",
            "Epoch: 5/30.. Training loss: 0.356.. Test loss: 0.339.. Test Accuracy: 0.877\n",
            "Epoch: 6/30.. Training loss: 0.341.. Test loss: 0.325.. Test Accuracy: 0.880\n",
            "Epoch: 7/30.. Training loss: 0.330.. Test loss: 0.322.. Test Accuracy: 0.883\n",
            "Epoch: 8/30.. Training loss: 0.320.. Test loss: 0.320.. Test Accuracy: 0.884\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xCmAldnrWKad",
        "colab_type": "text"
      },
      "source": [
        "**The 128 batch size**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W1PBZR_gUmP-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Define the network architecture\n",
        "from torch import nn, optim\n",
        "import torch.nn.functional as F\n",
        "\n",
        "trainloader = torch.utils.data.DataLoader(trainset, batch_size = 128, shuffle = True)\n",
        "testloader = torch.utils.data.DataLoader(testset, batch_size = 128, shuffle = True)\n",
        "\n",
        "start_time = time.time()\n",
        "\n",
        "model = nn.Sequential(nn.Linear(784, 256),\n",
        "                      nn.BatchNorm1d(256),\n",
        "                      nn.ReLU(),\n",
        "                      nn.Dropout(0.2),\n",
        "                      nn.Linear(256, 128),\n",
        "                      nn.BatchNorm1d(128),\n",
        "                      nn.ReLU(),\n",
        "                      nn.Dropout(0.2),\n",
        "                      nn.Linear(128, 64),\n",
        "                      nn.BatchNorm1d(64),\n",
        "                      nn.ReLU(),\n",
        "                      nn.Dropout(0.2),\n",
        "                      nn.Linear(64, 10),\n",
        "                      nn.LogSoftmax(dim = 1)\n",
        "                     )\n",
        "\n",
        "# Define the loss\n",
        "criterion = nn.NLLLoss()\n",
        "#criterion = nn.CrossEntropyLoss() we can use this one\n",
        "\n",
        "# Define the optimizer\n",
        "optimizer = optim.Adam(model.parameters(), lr = 0.002)\n",
        "\n",
        "# Define the epochs\n",
        "epochs = 30\n",
        "\n",
        "train_losses, test_losses = [], []\n",
        "\n",
        "for e in range(epochs):\n",
        "  running_loss = 0\n",
        "  for images, labels in trainloader:\n",
        "    # Flatten Fashion-MNIST images into a 784 long vector\n",
        "    images = images.view(images.shape[0], -1)\n",
        "    \n",
        "    # Training pass\n",
        "    optimizer.zero_grad()\n",
        "    \n",
        "    output = model.forward(images)\n",
        "    loss = criterion(output, labels)\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "    \n",
        "    running_loss += loss.item()\n",
        "  else:\n",
        "    test_loss = 0\n",
        "    accuracy = 0\n",
        "    \n",
        "    # Turn off gradients for validation, saves memory and computation\n",
        "    with torch.no_grad():\n",
        "      # Set the model to evaluation mode\n",
        "      model.eval()\n",
        "      \n",
        "      # Validation pass\n",
        "      for images, labels in testloader:\n",
        "        images = images.view(images.shape[0], -1)\n",
        "        log_ps = model(images)\n",
        "        test_loss += criterion(log_ps, labels)\n",
        "        \n",
        "        ps = torch.exp(log_ps)\n",
        "        top_p, top_class = ps.topk(1, dim = 1)\n",
        "        equals = top_class == labels.view(*top_class.shape)\n",
        "        accuracy += torch.mean(equals.type(torch.FloatTensor))\n",
        "    \n",
        "    model.train()\n",
        "    train_losses.append(running_loss/len(trainloader))\n",
        "    test_losses.append(test_loss/len(testloader))\n",
        "    \n",
        "    print(\"Epoch: {}/{}..\".format(e+1, epochs),\n",
        "          \"Training loss: {:.3f}..\".format(running_loss/len(trainloader)),\n",
        "          \"Test loss: {:.3f}..\".format(test_loss/len(testloader)),\n",
        "          \"Test Accuracy: {:.3f}\".format(accuracy/len(testloader)))\n",
        "    \n",
        "    \n",
        "    \n",
        "print(\"--- %s seconds with batch size = 128  ---\" % (time.time() - start_time))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6tJ0NOqyWTtu",
        "colab_type": "text"
      },
      "source": [
        "**A function to change learning rate**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d8J0vg3OMhPU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def adjust_learning_rate(optimizer, lr):\n",
        "    for param_group in optimizer.param_groups:\n",
        "        param_group['lr'] = lr\n",
        "    return optimizer"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yGkAaVpgMv70",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 538
        },
        "outputId": "3a10cad3-a25d-4afd-be27-9743c15e0dab"
      },
      "source": [
        "# Define the network architecture\n",
        "from torch import nn, optim\n",
        "import torch.nn.functional as F\n",
        "\n",
        "start_time = time.time()\n",
        "lr = 0.002\n",
        "model = nn.Sequential(nn.Linear(784, 256),\n",
        "                      nn.BatchNorm1d(256),\n",
        "                      nn.ReLU(),\n",
        "                      nn.Dropout(0.2),\n",
        "                      nn.Linear(256, 128),\n",
        "                      nn.BatchNorm1d(128),\n",
        "                      nn.ReLU(),\n",
        "                      nn.Dropout(0.2),\n",
        "                      nn.Linear(128, 64),\n",
        "                      nn.BatchNorm1d(64),\n",
        "                      nn.ReLU(),\n",
        "                      nn.Dropout(0.2),\n",
        "                      nn.Linear(64, 10),\n",
        "                      nn.LogSoftmax(dim = 1)\n",
        "                     )\n",
        "\n",
        "# Define the loss\n",
        "criterion = nn.NLLLoss()\n",
        "#criterion = nn.CrossEntropyLoss() we can use this one\n",
        "\n",
        "# Define the optimizer\n",
        "optimizer = optim.Adam(model.parameters(), lr = 0.002)\n",
        "\n",
        "#lambda1 = lambda e: e // 30\n",
        "#lambda2 = lambda e: 0.95 ** e\n",
        "#scheduler = torch.optim.lr_scheduler.LambdaLR(optimizer, lr_lambda=[lambda1, lambda2])\n",
        "\n",
        "\n",
        "# Define the epochs\n",
        "epochs = 30\n",
        "\n",
        "train_losses, test_losses = [], []\n",
        "\n",
        "for e in range(epochs):\n",
        "  lr  = lr* 0.1 ** (e // 10)\n",
        "  optimizer = adjust_learning_rate(optimizer, lr)\n",
        "  #schduler.step()\n",
        "  \n",
        "  \n",
        "  running_loss = 0\n",
        "  for images, labels in trainloader:\n",
        "    # Flatten Fashion-MNIST images into a 784 long vector\n",
        "    images = images.view(images.shape[0], -1)\n",
        "    \n",
        "    # Training pass\n",
        "    optimizer.zero_grad()\n",
        "    \n",
        "    output = model.forward(images)\n",
        "    loss = criterion(output, labels)\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "    \n",
        "    running_loss += loss.item()\n",
        "  else:\n",
        "    test_loss = 0\n",
        "    accuracy = 0\n",
        "    \n",
        "    # Turn off gradients for validation, saves memory and computation\n",
        "    with torch.no_grad():\n",
        "      # Set the model to evaluation mode\n",
        "      model.eval()\n",
        "      \n",
        "      # Validation pass\n",
        "      for images, labels in testloader:\n",
        "        images = images.view(images.shape[0], -1)\n",
        "        log_ps = model(images)\n",
        "        test_loss += criterion(log_ps, labels)\n",
        "        \n",
        "        ps = torch.exp(log_ps)\n",
        "        top_p, top_class = ps.topk(1, dim = 1)\n",
        "        equals = top_class == labels.view(*top_class.shape)\n",
        "        accuracy += torch.mean(equals.type(torch.FloatTensor))\n",
        "    \n",
        "    model.train()\n",
        "    train_losses.append(running_loss/len(trainloader))\n",
        "    test_losses.append(test_loss/len(testloader))\n",
        "    \n",
        "    print(\"Epoch: {}/{}..\".format(e+1, epochs),\n",
        "          \"Training loss: {:.3f}..\".format(running_loss/len(trainloader)),\n",
        "          \"Test loss: {:.3f}..\".format(test_loss/len(testloader)),\n",
        "          \"Test Accuracy: {:.3f}\".format(accuracy/len(testloader)))\n",
        "    \n",
        "    \n",
        "    \n",
        "print(\"--- %s seconds with learning rate changed ---\" % (time.time() - start_time))"
      ],
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch: 1/30.. Training loss: 0.573.. Test loss: 0.411.. Test Accuracy: 0.851\n",
            "Epoch: 2/30.. Training loss: 0.411.. Test loss: 0.371.. Test Accuracy: 0.865\n",
            "Epoch: 3/30.. Training loss: 0.372.. Test loss: 0.348.. Test Accuracy: 0.874\n",
            "Epoch: 4/30.. Training loss: 0.347.. Test loss: 0.333.. Test Accuracy: 0.877\n",
            "Epoch: 5/30.. Training loss: 0.330.. Test loss: 0.331.. Test Accuracy: 0.880\n",
            "Epoch: 6/30.. Training loss: 0.315.. Test loss: 0.319.. Test Accuracy: 0.887\n",
            "Epoch: 7/30.. Training loss: 0.303.. Test loss: 0.321.. Test Accuracy: 0.885\n",
            "Epoch: 8/30.. Training loss: 0.294.. Test loss: 0.324.. Test Accuracy: 0.885\n",
            "Epoch: 9/30.. Training loss: 0.286.. Test loss: 0.320.. Test Accuracy: 0.886\n",
            "Epoch: 10/30.. Training loss: 0.278.. Test loss: 0.308.. Test Accuracy: 0.890\n",
            "Epoch: 11/30.. Training loss: 0.238.. Test loss: 0.293.. Test Accuracy: 0.896\n",
            "Epoch: 12/30.. Training loss: 0.227.. Test loss: 0.290.. Test Accuracy: 0.898\n",
            "Epoch: 13/30.. Training loss: 0.225.. Test loss: 0.290.. Test Accuracy: 0.897\n",
            "Epoch: 14/30.. Training loss: 0.227.. Test loss: 0.289.. Test Accuracy: 0.897\n",
            "Epoch: 15/30.. Training loss: 0.225.. Test loss: 0.289.. Test Accuracy: 0.897\n",
            "Epoch: 16/30.. Training loss: 0.225.. Test loss: 0.291.. Test Accuracy: 0.896\n",
            "Epoch: 17/30.. Training loss: 0.227.. Test loss: 0.290.. Test Accuracy: 0.897\n",
            "Epoch: 18/30.. Training loss: 0.227.. Test loss: 0.290.. Test Accuracy: 0.897\n",
            "Epoch: 19/30.. Training loss: 0.227.. Test loss: 0.291.. Test Accuracy: 0.896\n",
            "Epoch: 20/30.. Training loss: 0.227.. Test loss: 0.289.. Test Accuracy: 0.898\n",
            "Epoch: 21/30.. Training loss: 0.227.. Test loss: 0.290.. Test Accuracy: 0.897\n",
            "Epoch: 22/30.. Training loss: 0.225.. Test loss: 0.288.. Test Accuracy: 0.897\n",
            "Epoch: 23/30.. Training loss: 0.223.. Test loss: 0.290.. Test Accuracy: 0.897\n",
            "Epoch: 24/30.. Training loss: 0.226.. Test loss: 0.291.. Test Accuracy: 0.896\n",
            "Epoch: 25/30.. Training loss: 0.225.. Test loss: 0.294.. Test Accuracy: 0.897\n",
            "Epoch: 26/30.. Training loss: 0.224.. Test loss: 0.292.. Test Accuracy: 0.897\n",
            "Epoch: 27/30.. Training loss: 0.226.. Test loss: 0.291.. Test Accuracy: 0.896\n",
            "Epoch: 28/30.. Training loss: 0.228.. Test loss: 0.289.. Test Accuracy: 0.897\n",
            "Epoch: 29/30.. Training loss: 0.225.. Test loss: 0.292.. Test Accuracy: 0.896\n",
            "Epoch: 30/30.. Training loss: 0.227.. Test loss: 0.291.. Test Accuracy: 0.897\n",
            "--- 639.0331268310547 seconds with batchnormalization added ---\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "j1-Wp3WYNq5_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}